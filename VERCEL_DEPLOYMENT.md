# Vercel Deployment Guide

## âœ… What's Been Set Up

Your app has been converted to work on Vercel's serverless platform:

1. **FastAPI Backend** (`api/index.py`)
   - Serverless-compatible functions
   - Embedded data for demo (can be replaced with cloud services)
   - Supports OpenAI API integration (optional)

2. **Frontend** (`public/index.html`)
   - Modern, responsive chat interface
   - Replaces Streamlit UI
   - Works with the FastAPI backend

3. **Vercel Configuration** (`vercel.json`)
   - Routes API calls to serverless functions
   - Serves static files from `/public`
   - Python 3.9 runtime

## ğŸš€ Deploy to Vercel

### Option 1: Deploy via Vercel CLI (Recommended)

```bash
# Install Vercel CLI
npm i -g vercel

# Login to Vercel
vercel login

# Deploy
vercel

# Follow prompts:
# - Set up and deploy? Yes
# - Which scope? (your account)
# - Link to existing project? No
# - Project name? multi-source-agent (or your choice)
# - Directory? ./
```

### Option 2: Deploy via GitHub

1. **Push to GitHub** (already done âœ…)
2. **Go to vercel.com**
3. **Import your repository**: `Navyareddyu/multi_source_agent`
4. **Configure:**
   - Framework Preset: Other
   - Root Directory: ./
   - Build Command: (leave empty)
   - Output Directory: (leave empty)
5. **Click Deploy**

## âš™ï¸ Optional: Add OpenAI API (For Better Responses)

To enable full LLM processing:

1. **Get OpenAI API Key** from https://platform.openai.com/api-keys
2. **In Vercel Dashboard:**
   - Go to your project â†’ Settings â†’ Environment Variables
   - Add: `OPENAI_API_KEY` = `your-api-key-here`
   - Redeploy

Without OpenAI, the app will return the context data directly (still functional, just without LLM processing).

## ğŸ“ Project Structure

```
multi_source_agent/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ index.py          # FastAPI serverless function
â”‚   â””â”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ public/
â”‚   â””â”€â”€ index.html        # Frontend UI
â”œâ”€â”€ vercel.json           # Vercel configuration
â””â”€â”€ .vercelignore         # Files to exclude
```

## ğŸ” How It Works

### Serverless Architecture

1. **Frontend** (`/`) â†’ Served as static HTML from `/public`
2. **API** (`/api/chat`) â†’ FastAPI function on Vercel serverless
3. **Data** â†’ Currently embedded (can be replaced with cloud DB)

### Request Flow

```
User types message
  â†“
Frontend sends POST to /api/chat
  â†“
Vercel invokes serverless function (api/index.py)
  â†“
Function processes query (SQL/document search)
  â†“
Returns response to frontend
  â†“
UI displays result
```

## ğŸ”„ Updating Data

### Current: Embedded Data
The app uses embedded data in `api/index.py`:
- `EMBEDDED_DB_DATA` - Database records
- `EMBEDDED_DOCUMENTS` - Policy documents

### Future: Cloud Services

To use cloud services, update `api/index.py`:

**For Database:**
```python
# Replace run_sql_query() with:
import psycopg2  # or your DB client
def run_sql_query(query: str) -> str:
    conn = psycopg2.connect(os.getenv("DATABASE_URL"))
    # ... execute query
```

**For Vector DB:**
```python
# Replace document_search() with:
import pinecone  # or chromadb cloud
def document_search(query: str) -> str:
    # ... cloud vector search
```

## ğŸ§ª Testing Locally

```bash
# Install dependencies
pip install -r api/requirements.txt

# Run FastAPI locally
cd api
uvicorn index:app --reload

# Open browser to http://localhost:8000
# Or serve the HTML file separately
```

## ğŸ“Š What Works Now

âœ… **SQL Queries**: Revenue, profit, Q1, Q2 data  
âœ… **Document Search**: Remote work policy, company strategy  
âœ… **Chat Interface**: Modern UI with message history  
âœ… **Serverless Ready**: Works on Vercel platform  
âœ… **Optional LLM**: Can integrate OpenAI for better responses  

## ğŸ› Troubleshooting

**Issue: "Module not found"**
- Check `api/requirements.txt` includes all dependencies
- Redeploy after updating requirements

**Issue: "Function timeout"**
- Increase `maxDuration` in `vercel.json` (max 60s on Pro plan)

**Issue: "CORS error"**
- Already handled in `api/index.py` with CORS middleware

**Issue: "404 on /api/chat"**
- Check `vercel.json` routes are correct
- Ensure `api/index.py` exists

## ğŸ“ Next Steps

1. Deploy to Vercel (see above)
2. Test the deployed app
3. (Optional) Add OpenAI API key for LLM processing
4. (Optional) Replace embedded data with cloud services

Your app is now ready for Vercel! ğŸ‰

